{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lujainAziz/LujainAlmajyul-it326/blob/main/Reports%20Folder/Phase3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 3 – Data Mining on Students Performance Dataset"
      ],
      "metadata": {
        "id": "VG0L9rVpyM8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Executive Summary\n",
        "\n",
        "Phase 3 of this project applied two core data mining techniques—Decision Tree classification and K-means clustering—to analyze the Students Performance Dataset. The goal was to predict student grade categories and uncover hidden behavioral patterns.\n",
        "\n",
        "The classification model showed that GPA, absences, study time, and parental support strongly influence academic performance. Meanwhile, clustering revealed three distinct groups of students that reflect real academic profiles: high performers, low performers, and average students.\n",
        "\n",
        "The results are highly interpretable and directly applicable to educational decision-making, providing a strong foundation for identifying at-risk students and enhancing learning strategies.\n"
      ],
      "metadata": {
        "id": "q9H-HuW_OBgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## [1] Problem\n",
        "\n",
        "The main problem addressed in this project is predicting and understanding students' academic performance. This is important because early identification of low-performing students helps educators provide timely support, improve learning strategies, and enhance academic outcomes.\n",
        "\n",
        "To solve this problem, we apply:\n",
        "\n",
        "- **Classification** to predict student grade categories.\n",
        "- **Clustering** to discover hidden behavioral patterns among students based on their characteristics.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "IEFoBpC4yPJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2] Data Mining Task\n",
        "\n",
        "We formalize the problem into two data mining tasks:\n",
        "\n",
        "### 2.1 Classification Task\n",
        "\n",
        "- **Goal:** Predict the `GradeClass` (A–F encoded as 0–4) for each student.\n",
        "- **Input features:** Demographic, behavioral, and academic variables such as:\n",
        "  - Study time\n",
        "  - Parental support\n",
        "  - Absences\n",
        "  - GPA\n",
        "  - Extracurricular activities and tutoring\n",
        "- **Output:** A predicted performance category for each student.\n",
        "\n",
        "### 2.2 Clustering Task\n",
        "\n",
        "- **Goal:** Group students into natural clusters based on their characteristics, without using the `GradeClass` label.\n",
        "- **Purpose:** Understand different profiles of students (e.g., high-performing, low-performing, average) and their behaviors to support decision-making in education.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "au9SWlfByQ2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [3] Data\n",
        "\n",
        "### **Dataset Name**\n",
        "Students Performance Dataset\n",
        "\n",
        "### **Dataset Source**\n",
        "https://www.kaggle.com/datasets/rabieelkharoua/students-performance-dataset\n",
        "\n",
        "### **Sample of the Raw Dataset**\n",
        "\n",
        "The sample shown below represents the original unmodified data before applying any preprocessing steps.\n"
      ],
      "metadata": {
        "id": "e4VQIQHIyZRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "# Clustering\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Clone repo if not exists\n",
        "repo_path = \"/content/LujainAlmajyul-it326\"\n",
        "\n",
        "# Load RAW dataset\n",
        "raw_df = pd.read_csv(\"/content/LujainAlmajyul-it326/Dataset/Raw_dataset.csv\")\n",
        "print(\"Sample of the raw dataset:\")\n",
        "raw_df.head()\n"
      ],
      "metadata": {
        "id": "yOw0pkMP24y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset Description**\n",
        "\n",
        "The dataset used in this project is the **Students Performance Dataset**, containing **2,392** student records and **15** original attributes that describe demographic, behavioral, and academic factors. The main attributes include:\n",
        "\n",
        "- **Demographic attributes**\n",
        "  - `Age`, `Gender`, `Ethnicity`\n",
        "- **Family background**\n",
        "  - `ParentalEducation`, `ParentalSupport`\n",
        "- **Learning indicators**\n",
        "  - `StudyTimeWeekly`, `Absences`, `Tutoring`\n",
        "- **Activities**\n",
        "  - `Extracurricular`, `Sports`, `Music`, `Volunteering`\n",
        "- **Academic performance**\n",
        "  - `GPA`, `GradeClass` (target class label)\n",
        "\n",
        "The `GradeClass` attribute is encoded as five categories:\n",
        "\n",
        "- **4 → A** (1211 students)\n",
        "- **3 → B** (414 students)\n",
        "- **2 → C** (391 students)\n",
        "- **1 → D** (269 students)\n",
        "- **0 → F** (107 students)\n",
        "\n",
        "This distribution shows that the dataset is **imbalanced**, with high-performing students (Class 4) forming the majority class, while failing students (Class 0) are under-represented.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "I9dNFFM52zbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [4] Data Preprocessing\n",
        "\n",
        "Several preprocessing steps were applied to improve data quality and prepare the dataset for machine learning models.\n",
        "\n",
        "\n",
        "### **4.1 Missing Values Check**\n",
        "\n",
        "- The dataset contains **no missing values** in any column.\n",
        "- Therefore, no imputation or removal of rows was necessary.\n",
        "\n",
        "### **4.2 Noise & Outlier Detection**\n",
        "\n",
        "- Outliers were examined using:\n",
        "  - Boxplots\n",
        "  - IQR method\n",
        "- The following features were checked:\n",
        "  - `GPA`\n",
        "  - `StudyTimeWeekly`\n",
        "  - `Absences`\n",
        "- No significant extreme outliers were found, so no removal or capping was required.\n",
        "\n",
        "\n",
        "### **4.3 Variable Transformation**\n",
        "\n",
        "- The `StudyTimeWeekly` feature showed slight skewness.\n",
        "- A **Yeo–Johnson PowerTransformer** was applied to normalize its distribution and stabilize variance.\n",
        "- The transformed version is stored as:\n",
        "  - `StudyTimeWeekly_transformed`\n",
        "\n",
        "\n",
        "### **4.4 Scaling / Normalization**\n",
        "\n",
        "- The features `GPA` and `Absences` have different numeric ranges.\n",
        "- To balance their contribution to the model, both were normalized using **StandardScaler**, resulting in:\n",
        "  - `GPA_scaled`\n",
        "  - `Absences_scaled`\n",
        "\n",
        "\n",
        "### **4.5 Discretization**\n",
        "\n",
        "- To improve interpretability of classification results, the `GPA` feature was discretized into four bins using **quantile-based KBinsDiscretizer**.\n",
        "- The resulting feature:\n",
        "  - `GPA_binned` (values range from 0 to 3 representing increasing GPA levels)\n",
        "\n",
        "\n",
        "### **4.6 Encoding**\n",
        "\n",
        "- All categorical variables in the raw dataset were already provided as numerical codes, including:\n",
        "  - `Gender` → 0 (Male), 1 (Female)\n",
        "  - `ParentalEducation` → 0–4\n",
        "  - `ParentalSupport` → 0–4\n",
        "  - Activity indicators → 0 (No), 1 (Yes)\n",
        "- Therefore, **no additional label encoding** was necessary.\n",
        "\n",
        "\n",
        "\n",
        "### **4.7 Correlation Analysis**\n",
        "\n",
        "- A correlation heatmap was generated to study relationships among variables.\n",
        "- Key findings:\n",
        "  - **GPA** is strongly negatively correlated with **Absences**.\n",
        "  - **GPA** positively correlates with **ParentalSupport** and **StudyTimeWeekly**.\n",
        "  - **GradeClass** strongly correlates with **GPA**, validating the label’s meaning.\n",
        "\n",
        "\n",
        "\n",
        "### **4.8 Final Preprocessed Dataset**\n",
        "\n",
        "After preprocessing, the dataset includes all original attributes plus the engineered features:\n",
        "\n",
        "- `StudyTimeWeekly_transformed`\n",
        "- `GPA_scaled`\n",
        "- `Absences_scaled`\n",
        "- `GPA_binned`\n",
        "\n",
        "These processed features were used for all classification and clustering tasks in Phase 3.\n",
        "\n",
        "\n",
        "\n",
        "### **Snapshot After Preprocessing**\n",
        "\n",
        "This snapshot confirms that transformation, scaling, and discretization were applied successfully."
      ],
      "metadata": {
        "id": "5Vg-vWouyeUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load PREPROCESSED dataset\n",
        "df = pd.read_csv(\"/content/LujainAlmajyul-it326/Dataset/Preprocessed_dataset.csv\")\n",
        "print(\"Snapshot after preprocessing:\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "v7T_J3hX3b87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Y_nZe_Qx3ecz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [5] Data Mining Technique\n",
        "\n",
        "In this phase, we apply two main data mining techniques to the Students Performance Dataset.\n",
        "\n",
        "### 5.1 Decision Tree Classification\n",
        "\n",
        "We use the **DecisionTreeClassifier** from `sklearn.tree` to predict the `GradeClass` attribute, which represents student performance levels (0–4, corresponding to F, D, C, B, and A).\n",
        "\n",
        "**Input features (X):**\n",
        "\n",
        "- Age, Gender, Ethnicity, ParentalEducation  \n",
        "- StudyTimeWeekly_transformed, Absences_scaled  \n",
        "- Tutoring, ParentalSupport  \n",
        "- Extracurricular, Sports, Music, Volunteering  \n",
        "- GPA_scaled  \n",
        "\n",
        "**Target (y):**\n",
        "\n",
        "- `GradeClass` (0–4)\n",
        "\n",
        "**Attribute selection measures (splitting criteria):**\n",
        "\n",
        "- **Gini index** (`criterion=\"gini\"`)\n",
        "- **Information Gain (Entropy)** (`criterion=\"entropy\"`)\n",
        "\n",
        "**Train–test partitions:**\n",
        "\n",
        "We evaluate three train–test splits:\n",
        "\n",
        "- 60% training – 40% testing  \n",
        "- 70% training – 30% testing  \n",
        "- 80% training – 20% testing  \n",
        "\n",
        "For each configuration, we compute:\n",
        "\n",
        "- Accuracy  \n",
        "- Macro-averaged Precision  \n",
        "- Macro-averaged Recall  \n",
        "- Macro-averaged F1-score  \n",
        "- Confusion Matrix  \n",
        "\n",
        "One final Decision Tree is visualized to interpret the classification rules.\n",
        "\n",
        "### 5.2 K-means Clustering\n",
        "\n",
        "We use the **KMeans** algorithm from `sklearn.cluster` to discover groups of students with similar characteristics, **without using** the `GradeClass` label (unsupervised learning).\n",
        "\n",
        "**Clustering features:**\n",
        "\n",
        "- Age, Gender, Ethnicity, ParentalEducation  \n",
        "- StudyTimeWeekly_transformed, Absences  \n",
        "- Tutoring, ParentalSupport  \n",
        "- Extracurricular, Sports, Music, Volunteering  \n",
        "- GPA  \n",
        "\n",
        "All features are standardized using **StandardScaler** before clustering.\n",
        "\n",
        "**Evaluation of K:**\n",
        "\n",
        "- We test values of **K from 2 to 10**.  \n",
        "- For each K, we compute:  \n",
        "  - **Elbow Method:** Total Within-Cluster Sum of Squares (WCSS)  \n",
        "  - **Silhouette Coefficient:** Measures cohesion and separation of clusters  \n",
        "\n",
        "We then select three candidate K values (e.g., K = 2, 3, 4) for detailed comparison and visualize the clusters in 2D using **PCA (Principal Component Analysis)**.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LkujJvhGyjsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [6] Evaluation and Comparison\n",
        "\n",
        "### 6.1 Classification – Decision Tree\n",
        "\n",
        "We apply Decision Tree classification to predict `GradeClass` using multiple demographic, behavioral, and academic features.\n",
        "### 6.1.1 Feature Definition\n",
        "\n",
        "We select the engineered and scaled features for training the Decision Tree classifier."
      ],
      "metadata": {
        "id": "1LQ-LsyE66bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Classification: Define features (X) and target (y) =====\n",
        "\n",
        "feature_cols_clf = [\n",
        "    \"Age\", \"Gender\", \"Ethnicity\", \"ParentalEducation\",\n",
        "    \"StudyTimeWeekly_transformed\",\n",
        "    \"Absences_scaled\",\n",
        "    \"Tutoring\", \"ParentalSupport\",\n",
        "    \"Extracurricular\", \"Sports\", \"Music\", \"Volunteering\",\n",
        "    \"GPA_scaled\"\n",
        "]\n",
        "\n",
        "X = df[feature_cols_clf].copy()\n",
        "y = df[\"GradeClass\"].astype(int)\n",
        "\n",
        "print(\"\\nClass Distribution:\")\n",
        "\n",
        "class_dist = y.value_counts().sort_index().to_frame()\n",
        "class_dist.columns = ['Count']\n",
        "class_dist.index.name = 'GradeClass'\n",
        "class_dist['Percentage'] = (class_dist['Count'] / class_dist['Count'].sum() * 100).round(2)\n",
        "\n",
        "display(class_dist)"
      ],
      "metadata": {
        "id": "fz-r5-yd7OEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1.2 Model Training\n",
        "\n",
        "We evaluate the classifier using three train–test splits (60–40, 70–30, 80–20)  \n",
        "and two splitting criteria (Gini, Entropy).  \n",
        "We compute accuracy, precision, recall, F1-score, and confusion matrices for each model."
      ],
      "metadata": {
        "id": "pMs8-ACb7Szw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Full Classification Experiment Block (creates metrics_df and cm_long_df) =====\n",
        "\n",
        "partition_sizes = [0.6, 0.7, 0.8]\n",
        "criteria = [\"gini\", \"entropy\"]\n",
        "\n",
        "metrics_rows = []\n",
        "cm_rows = []\n",
        "labels = sorted(y.unique())\n",
        "\n",
        "for train_size in partition_sizes:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        train_size=train_size,\n",
        "        random_state=42,\n",
        "        stratify=y\n",
        "    )\n",
        "\n",
        "    for crit in criteria:\n",
        "        clf = DecisionTreeClassifier(\n",
        "            criterion=crit,\n",
        "            random_state=42\n",
        "        )\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = clf.predict(X_test)\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        report = classification_report(\n",
        "            y_test, y_pred, output_dict=True, zero_division=0\n",
        "        )\n",
        "        macro = report[\"macro avg\"]\n",
        "\n",
        "        metrics_rows.append({\n",
        "            \"train_size\": f\"{int(train_size*100)}%\",\n",
        "            \"criterion\": crit,\n",
        "            \"accuracy\": acc,\n",
        "            \"precision_macro\": macro[\"precision\"],\n",
        "            \"recall_macro\": macro[\"recall\"],\n",
        "            \"f1_macro\": macro[\"f1-score\"]\n",
        "        })\n",
        "\n",
        "        cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "\n",
        "        for i, true_lbl in enumerate(labels):\n",
        "            for j, pred_lbl in enumerate(labels):\n",
        "                cm_rows.append({\n",
        "                    \"train_size\": f\"{int(train_size*100)}%\",\n",
        "                    \"criterion\": crit,\n",
        "                    \"true_label\": true_lbl,\n",
        "                    \"pred_label\": pred_lbl,\n",
        "                    \"count\": cm[i, j]\n",
        "                })\n",
        "\n",
        "# Convert to DataFrames\n",
        "metrics_df = pd.DataFrame(metrics_rows)\n",
        "cm_long_df = pd.DataFrame(cm_rows)\n",
        "\n",
        "print(\"✓ metrics_df and cm_long_df successfully created\")"
      ],
      "metadata": {
        "id": "Y6EV7h9G7WuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1.3 Accuracy Table\n",
        "\n",
        "The following table compares accuracy across criteria and train–test splits."
      ],
      "metadata": {
        "id": "ixJRsc1D8YNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df = pd.DataFrame(metrics_rows)\n",
        "\n",
        "acc_table = metrics_df.pivot(\n",
        "    index=\"train_size\",\n",
        "    columns=\"criterion\",\n",
        "    values=\"accuracy\"\n",
        ")\n",
        "\n",
        "acc_table"
      ],
      "metadata": {
        "id": "xV3pGXqa7eUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1.4 Interpretation of Accuracy\n",
        "\n",
        "- Best accuracy: **Gini @ 80% training = 0.868476**  \n",
        "- Entropy slightly lower at the same split  \n",
        "- Accuracy improves with larger training sizes  \n",
        "- Gini outperforms entropy overall"
      ],
      "metadata": {
        "id": "6nx5UiVw7tGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1.5 Confusion Matrices\n",
        "Confusion matrices are generated for all splits and criteria."
      ],
      "metadata": {
        "id": "_ehhyfR88MOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Confusion Matrices (as tables) for each configuration =====\n",
        "\n",
        "labels = sorted(y.unique())\n",
        "\n",
        "for ts in sorted(cm_long_df[\"train_size\"].unique()):\n",
        "    for crit in sorted(cm_long_df[\"criterion\"].unique()):\n",
        "\n",
        "        sub = cm_long_df[\n",
        "            (cm_long_df[\"train_size\"] == ts) &\n",
        "            (cm_long_df[\"criterion\"] == crit)\n",
        "        ]\n",
        "\n",
        "        if sub.empty:\n",
        "            continue\n",
        "\n",
        "        print(f\"Confusion Matrix – train_size={ts}, criterion={crit}\")\n",
        "\n",
        "        cm_table = sub.pivot(\n",
        "            index=\"true_label\",\n",
        "            columns=\"pred_label\",\n",
        "            values=\"count\"\n",
        "        ).reindex(index=labels, columns=labels)\n",
        "\n",
        "        cm_table.index = [f\"True_{l}\" for l in cm_table.index]\n",
        "        cm_table.columns = [f\"Pred_{l}\" for l in cm_table.columns]\n",
        "\n",
        "        display(cm_table)"
      ],
      "metadata": {
        "id": "d__1Hzha7rUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1.6 Decision Tree Visualization\n",
        "A final Decision Tree is visualized for interpretability."
      ],
      "metadata": {
        "id": "gHsqEW7U8QhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Classification: visualize one final Decision Tree =====\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    train_size=0.7,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "best_clf = DecisionTreeClassifier(\n",
        "    criterion=\"gini\",\n",
        "    random_state=42,\n",
        "    max_depth=4\n",
        ")\n",
        "best_clf.fit(X_train, y_train)\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(\n",
        "    best_clf,\n",
        "    feature_names=feature_cols_clf,\n",
        "    class_names=[str(c) for c in sorted(y.unique())],\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    fontsize=8\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vBCFRVvz8d9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Clustering – K-means\n",
        "\n",
        "We apply K-means clustering to discover student groups with similar characteristics **without using the GradeClass label**.  \n",
        "All numeric features are standardized before clustering to ensure equal contribution."
      ],
      "metadata": {
        "id": "8bpQ3ooS9drV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Clustering: Prepare features (remove label) and scale =====\n",
        "\n",
        "# Drop class label for unsupervised learning\n",
        "X_clu = df.drop(\"GradeClass\", axis=1)\n",
        "\n",
        "# Keep only numeric columns (dataset is already numeric, but this is safe)\n",
        "X_clu = X_clu.select_dtypes(include=np.number)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_clu_scaled = scaler.fit_transform(X_clu)\n",
        "\n",
        "print(\"Clustering features:\", list(X_clu.columns))\n",
        "print(\"Scaled shape:\", X_clu_scaled.shape)"
      ],
      "metadata": {
        "id": "lEjmhHHx9fp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2.1 Elbow and Silhouette Analysis\n",
        "\n",
        "We test values of **K from 2 to 10**.  \n",
        "For each K, we compute:\n",
        "- **WCSS (Elbow Method)** to measure cluster compactness.\n",
        "- **Silhouette Score** to measure separation and cohesion.\n",
        "\n",
        "The results are stored in a table for comparison."
      ],
      "metadata": {
        "id": "qe3VfRiA9iXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Clustering: Elbow and Silhouette for K = 2..10 =====\n",
        "\n",
        "K_range = range(2, 11)\n",
        "k_rows = []\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels_k = kmeans.fit_predict(X_clu_scaled)\n",
        "\n",
        "    inertia = kmeans.inertia_              # WCSS\n",
        "    sil = silhouette_score(X_clu_scaled, labels_k)\n",
        "\n",
        "    k_rows.append({\n",
        "        \"K\": k,\n",
        "        \"WCSS\": inertia,\n",
        "        \"Silhouette\": sil\n",
        "    })\n",
        "\n",
        "k_all_df = pd.DataFrame(k_rows)\n",
        "k_all_df"
      ],
      "metadata": {
        "id": "agcfI8Sd9k86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interpretation of Elbow & Silhouette\n",
        "\n",
        "- The Elbow curve starts flattening around **K = 4**, suggesting diminishing returns after that point.\n",
        "- The highest Silhouette score occurs at **K = 3**.\n",
        "\n",
        "**Conclusion:**\n",
        "- Elbow suggests **K ≈ 4**\n",
        "- Silhouette suggests **K = 3**\n",
        "- Based on majority rule and better separation, **BEST_K = 3**."
      ],
      "metadata": {
        "id": "pKZOIHxw9myJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Helper to identify best K by Silhouette =====\n",
        "best_k_row = k_all_df.loc[k_all_df[\"Silhouette\"].idxmax()]\n",
        "BEST_K = int(best_k_row[\"K\"])\n",
        "BEST_K"
      ],
      "metadata": {
        "id": "CQvwjpgc9skF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Elbow and Silhouette Plots\n",
        "\n",
        "We visualize WCSS and Silhouette across K values to support selecting the optimal number of clusters."
      ],
      "metadata": {
        "id": "jryNb4jk-LCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Elbow Method Plot =====\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(k_all_df[\"K\"], k_all_df[\"WCSS\"], marker='o')\n",
        "plt.xlabel(\"K\")\n",
        "plt.ylabel(\"WCSS\")\n",
        "plt.title(\"Elbow Method\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ===== Silhouette Plot =====\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(k_all_df[\"K\"], k_all_df[\"Silhouette\"], marker='o')\n",
        "plt.xlabel(\"K\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.title(\"Silhouette Coefficient for K = 2..10\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FAhpamy_-O3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2.2 Detailed Comparison for Selected K Values\n",
        "\n",
        "Based on Elbow and Silhouette, we select three candidate values  \n",
        "(e.g., K = 2, 3, 4) for deeper comparison."
      ],
      "metadata": {
        "id": "k-p2GA1u-Qwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Detailed comparison for selected K values =====\n",
        "chosen_K = [2, 3, 4]\n",
        "\n",
        "cluster_details = []\n",
        "\n",
        "for k in chosen_K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels_k = kmeans.fit_predict(X_clu_scaled)\n",
        "\n",
        "    inertia = kmeans.inertia_\n",
        "    sil = silhouette_score(X_clu_scaled, labels_k)\n",
        "\n",
        "    cluster_details.append({\n",
        "        \"K\": k,\n",
        "        \"WCSS\": inertia,\n",
        "        \"Silhouette\": sil\n",
        "    })\n",
        "\n",
        "cluster_results_df = pd.DataFrame(cluster_details)\n",
        "cluster_results_df"
      ],
      "metadata": {
        "id": "UosoQnQQ-SaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PCA Preparation\n",
        "\n",
        "We reduce the scaled features into 2D using PCA to visualize cluster separation."
      ],
      "metadata": {
        "id": "_TzRvae3-UWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== PCA for 2D visualization =====\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_clu_pca = pca.fit_transform(X_clu_scaled)\n",
        "\n",
        "print(\"PCA shape:\", X_clu_pca.shape)"
      ],
      "metadata": {
        "id": "Qdibm0y1-WHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cluster Visualization (2D PCA)\n",
        "\n",
        "We visualize clusters for K = 2, 3, and 4 in PCA space to interpret their structure."
      ],
      "metadata": {
        "id": "do1EAH2h-YiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== PCA visualization for chosen K values =====\n",
        "for k in chosen_K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels_k = kmeans.fit_predict(X_clu_scaled)\n",
        "\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    plt.scatter(X_clu_pca[:, 0], X_clu_pca[:, 1], c=labels_k, cmap='viridis')\n",
        "    plt.title(f\"K-means Clusters (K = {k}) in PCA 2D Space\")\n",
        "    plt.xlabel(\"Principal Component 1\")\n",
        "    plt.ylabel(\"Principal Component 2\")\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6kIdmfdt-aIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interpretation of Clustering Results (K = 3)\n",
        "\n",
        "For the optimal K = 3, the clusters show meaningful student groupings:\n",
        "\n",
        "1. **High-performing cluster**\n",
        "   - High GPA, low absences, higher study time, strong parental support.\n",
        "   - Likely corresponds to GradeClass 3–4.\n",
        "\n",
        "2. **Low-performing cluster**\n",
        "   - Low GPA, high absences, lower study time.\n",
        "   - Likely corresponds to GradeClass 0–1.\n",
        "\n",
        "3. **Medium-performing cluster**\n",
        "   - Moderate GPA and absences with average study habits.\n",
        "   - Represents students between low and high performance (GradeClass 2–3)."
      ],
      "metadata": {
        "id": "m8BtS73a-fx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Conclusion for K-means Clustering\n",
        "\n",
        "Based on WCSS and Silhouette analysis, the optimal number of clusters is **K = 3**.  \n",
        "This value provides the best balance between compact clusters (low WCSS)  \n",
        "and strong separation (high Silhouette score).\n",
        "\n",
        "The resulting clusters reveal clear student performance patterns:\n",
        "- A high-performing group\n",
        "- A low-performing group\n",
        "- A medium group representing average performance\n",
        "\n",
        "These insights support understanding student behavior and identifying groups needing academic intervention."
      ],
      "metadata": {
        "id": "ImBz7B5x-jXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# [7] Findings and Discussion\n",
        "\n",
        "## 7.1 Classification Findings\n",
        "\n",
        "The Decision Tree classifier was evaluated using three train–test partitions (60–40, 70–30, and 80–20) and two attribute selection measures (Gini and entropy).\n",
        "\n",
        "The accuracy table shows that:\n",
        "The highest accuracy was achieved using 80% training data with the Gini criterion (0.868476),\n",
        "followed closely by 80% training with Entropy (0.860125). The 70% training partition with\n",
        "Gini also performed well (0.855153).\n",
        "- The differences in accuracy between Gini and entropy are small, but one of them slightly outperforms the other depending on the partition.\n",
        "- The confusion matrices reveal that some GradeClass categories, especially the minority classes, are more difficult to predict.\n",
        "\n",
        "The decision tree visualization confirms that **Absences**, **GPA**, and **ParentalSupport** are key factors. Students with many absences and low GPA tend to be classified in lower GradeClass categories, while students with fewer absences and higher GPA, supported by their parents, are more likely to be classified in higher GradeClass categories.\n",
        "\n",
        "## 7.2 Clustering Findings\n",
        "\n",
        "K-means clustering was applied with K values from 2 to 10. Using both WCSS (Elbow Method) and Silhouette scores:\n",
        "\n",
        "- The Elbow plot suggests **K ≈ 4** as a good balance between model complexity and within-cluster variance.\n",
        "- The Silhouette scores indicate that **K = 3** provides the best cluster separation.\n",
        "\n",
        "The final clusters reveal distinct student groups:\n",
        "- A high-performance group: high GPA, high study time, low absences, good parental support.\n",
        "- A low-performance group: low GPA, low study time, many absences, weak parental support.\n",
        "- One or more intermediate groups with mixed or average characteristics.\n",
        "\n",
        "## 7.3 Overall Discussion\n",
        "\n",
        "Combining classification and clustering provides a comprehensive understanding of student performance:\n",
        "\n",
        "- **Classification (Decision Tree)** offers an interpretable predictive model that can be used to automatically identify at-risk students based on their characteristics.\n",
        "- **Clustering (K-means)** uncovers hidden patterns and groups of students with similar behavior, which can help educators design targeted support strategies for each cluster.\n",
        "\n",
        "These results are aligned with educational literature, which emphasizes the importance of attendance, study effort, and family support in academic success.\n",
        "\n",
        "\n",
        "## Comparison with Related Research\n",
        "\n",
        "Our findings align closely with the research conducted by Baradwaj and Pal (2011),\n",
        "who applied Decision Tree classification (ID3 algorithm) to predict student performance\n",
        "in higher education. Their study, which analyzed 50 students from VBS Purvanchal University,\n",
        "India, demonstrated that attendance, class test grades, and previous semester marks were\n",
        "the most influential predictors of end-semester performance.\n",
        "\n",
        "**Similarities with our results:**\n",
        "\n",
        "1. **Attendance as a Critical Factor:**\n",
        "   Baradwaj and Pal's research confirmed that attendance strongly correlates with\n",
        "   academic success, with their decision tree placing attendance (ATT) as a high-priority\n",
        "   splitting criterion. Our analysis similarly found that Absences has a strong negative\n",
        "   correlation with GPA (-0.92), making it one of the top predictors in our Decision Tree model.\n",
        "\n",
        "2. **Previous Academic Performance:**\n",
        "   In their study, Previous Semester Marks (PSM) served as the root node with the highest\n",
        "   information gain (0.577). Similarly, our model identified GPA_scaled as the most\n",
        "   influential feature at the top level of the tree, confirming that prior academic\n",
        "   achievement is the strongest indicator of future performance.\n",
        "\n",
        "3. **Study Habits and Support:**\n",
        "   While Baradwaj and Pal focused on class test grades and assignments, our dataset\n",
        "   extended this to include StudyTimeWeekly and ParentalSupport, both of which showed\n",
        "   significant positive correlations with student success. This aligns with their\n",
        "   conclusion that consistent academic activities improve performance.\n",
        "\n",
        "**Key Differences:**\n",
        "\n",
        "- **Dataset Scale:** Our study analyzed 2,392 students compared to their 50 students,\n",
        "  providing more robust and generalizable results.\n",
        "- **Class Imbalance:** Our dataset exhibited significant class imbalance (1,211 students\n",
        "  in GradeClass 4 vs. 107 in GradeClass 0), which required careful evaluation of minority\n",
        "  class predictions—an issue not addressed in their smaller, more balanced dataset.\n",
        "- **Clustering Analysis:** Unlike Baradwaj and Pal, who focused solely on classification,\n",
        "  we also applied K-means clustering to discover hidden behavioral patterns. Our clustering\n",
        "  results (K=3) revealed three distinct student profiles: high performers, low performers,\n",
        "  and medium performers, providing additional insights beyond classification alone.\n",
        "\n",
        "**Validation of Educational Principles:**\n",
        "\n",
        "Both studies confirm the importance of attendance, prior performance, and consistent\n",
        "engagement in predicting student success. Baradwaj and Pal's IF-THEN rules, such as:\n",
        "\"IF PSM = 'First' AND ATT = 'Good' AND CTG = 'Good' THEN ESM = 'First'\" mirror our\n",
        "Decision Tree findings, where students with high GPA, low absences, and strong study\n",
        "habits are classified into higher grade categories.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "Our results validate and extend the findings of Baradwaj and Pal (2011) by demonstrating\n",
        "that data mining techniques—particularly Decision Trees—can effectively predict student\n",
        "performance across different educational contexts and dataset sizes. The consistency\n",
        "between our findings and their research reinforces the reliability of these methods for\n",
        "educational data mining and supports their application in early identification of at-risk students."
      ],
      "metadata": {
        "id": "okec_hTL0cJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Conclusion of Phase 3\n",
        "\n",
        "The application of Decision Tree classification and K-means clustering on the Students Performance Dataset provided valuable and complementary insights into student behavior and academic outcomes. Decision Tree classification achieved strong accuracy (up to 0.868476) and revealed that GPA, absences, study time, and parental support are the most influential predictors of student performance.\n",
        "\n",
        "Clustering results showed that K = 3 is the optimal number of clusters based on the combined Elbow and Silhouette analyses. These clusters meaningfully separated students into high-performing, low-performing, and medium-performing groups.\n",
        "\n",
        "Overall, the results demonstrate that data mining techniques can effectively support academic institutions in early detection of at-risk students, understanding key performance factors, and designing targeted interventions to enhance student success.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "xO-gVRZYN_J9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [8] References\n",
        "\n",
        "[1] J. Han, M. Kamber, and J. Pei, *Data Mining: Concepts and Techniques*, 3rd ed. Morgan Kaufmann, 2011.\n",
        "\n",
        "[2] B. K. Baradwaj and S. Pal, \"Mining Educational Data to Analyze Students' Performance,\"  \n",
        "    *International Journal of Advanced Computer Science and Applications*, vol. 2, no. 6,  \n",
        "    pp. 63–69, 2011.\n",
        "\n",
        "[3] A. R. Kizilcec, “Understanding Student Behavioral Patterns in Online Learning,”  \n",
        "    *IEEE Transactions on Learning Technologies*, vol. 13, no. 3, pp. 451–463, 2020.\n"
      ],
      "metadata": {
        "id": "1FOXH7DM0e12"
      }
    }
  ]
}