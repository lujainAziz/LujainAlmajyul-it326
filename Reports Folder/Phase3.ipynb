{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lujainAziz/LujainAlmajyul-it326/blob/main/Reports%20Folder/Phase3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 3 – Data Mining on Students Performance Dataset"
      ],
      "metadata": {
        "id": "VG0L9rVpyM8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Executive Summary\n",
        "\n",
        "Phase 3 of this project applied two core data mining techniques—Decision Tree classification and K-means clustering—to analyze the Students Performance Dataset. The goal was to predict student grade categories and uncover hidden behavioral patterns.\n",
        "\n",
        "The classification model showed that GPA, absences, study time, and parental support strongly influence academic performance. Meanwhile, clustering revealed three distinct groups of students that reflect real academic profiles: high performers, low performers, and average students.\n",
        "\n",
        "The results are highly interpretable and directly applicable to educational decision-making, providing a strong foundation for identifying at-risk students and enhancing learning strategies.\n"
      ],
      "metadata": {
        "id": "q9H-HuW_OBgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## [1] Problem\n",
        "\n",
        "The main problem addressed in this project is predicting and understanding students' academic performance. This is important because early identification of low-performing students helps educators provide timely support, improve learning strategies, and enhance academic outcomes.\n",
        "\n",
        "To solve this problem, we apply:\n",
        "\n",
        "- **Classification** to predict student grade categories.\n",
        "- **Clustering** to discover hidden behavioral patterns among students based on their characteristics.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "IEFoBpC4yPJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2] Data Mining Task\n",
        "\n",
        "We formalize the problem into two data mining tasks:\n",
        "\n",
        "### 2.1 Classification Task\n",
        "\n",
        "- **Goal:** Predict the `GradeClass` (A–F encoded as 0–4) for each student.\n",
        "- **Input features:** Demographic, behavioral, and academic variables such as:\n",
        "  - Study time\n",
        "  - Parental support\n",
        "  - Absences\n",
        "  - GPA\n",
        "  - Extracurricular activities and tutoring\n",
        "- **Output:** A predicted performance category for each student.\n",
        "\n",
        "### 2.2 Clustering Task\n",
        "\n",
        "- **Goal:** Group students into natural clusters based on their characteristics, without using the `GradeClass` label.\n",
        "- **Purpose:** Understand different profiles of students (e.g., high-performing, low-performing, average) and their behaviors to support decision-making in education.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "au9SWlfByQ2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [3] Data\n",
        "\n",
        "### **Dataset Name**\n",
        "Students Performance Dataset\n",
        "\n",
        "### **Dataset Source**\n",
        "https://www.kaggle.com/datasets/rabieelkharoua/students-performance-dataset\n",
        "\n",
        "### **Sample of the Raw Dataset**\n",
        "\n",
        "The sample shown below represents the original unmodified data before applying any preprocessing steps.\n"
      ],
      "metadata": {
        "id": "e4VQIQHIyZRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display sample of raw dataset\n",
        "raw_df = pd.read_csv(\"Dataset/Raw_dataset.csv\")\n",
        "print(\"Sample of the raw dataset:\")\n",
        "raw_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "yOw0pkMP24y7",
        "outputId": "e49665ee-077a-4cbf-c522-e15c1f75e7d4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-653180681.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Display sample of raw dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mraw_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset/Raw_dataset.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample of the raw dataset:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mraw_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset Description**\n",
        "\n",
        "The dataset used in this project is the **Students Performance Dataset**, containing **2,392** student records and **15** original attributes that describe demographic, behavioral, and academic factors. The main attributes include:\n",
        "\n",
        "- **Demographic attributes**\n",
        "  - `Age`, `Gender`, `Ethnicity`\n",
        "- **Family background**\n",
        "  - `ParentalEducation`, `ParentalSupport`\n",
        "- **Learning indicators**\n",
        "  - `StudyTimeWeekly`, `Absences`, `Tutoring`\n",
        "- **Activities**\n",
        "  - `Extracurricular`, `Sports`, `Music`, `Volunteering`\n",
        "- **Academic performance**\n",
        "  - `GPA`, `GradeClass` (target class label)\n",
        "\n",
        "The `GradeClass` attribute is encoded as five categories:\n",
        "\n",
        "- **4 → A** (1211 students)\n",
        "- **3 → B** (414 students)\n",
        "- **2 → C** (391 students)\n",
        "- **1 → D** (269 students)\n",
        "- **0 → F** (107 students)\n",
        "\n",
        "This distribution shows that the dataset is **imbalanced**, with high-performing students (Class 4) forming the majority class, while failing students (Class 0) are under-represented.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "I9dNFFM52zbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [4] Data Preprocessing\n",
        "\n",
        "Several preprocessing steps were applied to improve data quality and prepare the dataset for machine learning models.\n",
        "\n",
        "\n",
        "### **4.1 Missing Values Check**\n",
        "\n",
        "- The dataset contains **no missing values** in any column.\n",
        "- Therefore, no imputation or removal of rows was necessary.\n",
        "\n",
        "### **4.2 Noise & Outlier Detection**\n",
        "\n",
        "- Outliers were examined using:\n",
        "  - Boxplots\n",
        "  - IQR method\n",
        "- The following features were checked:\n",
        "  - `GPA`\n",
        "  - `StudyTimeWeekly`\n",
        "  - `Absences`\n",
        "- No significant extreme outliers were found, so no removal or capping was required.\n",
        "\n",
        "\n",
        "### **4.3 Variable Transformation**\n",
        "\n",
        "- The `StudyTimeWeekly` feature showed slight skewness.\n",
        "- A **Yeo–Johnson PowerTransformer** was applied to normalize its distribution and stabilize variance.\n",
        "- The transformed version is stored as:\n",
        "  - `StudyTimeWeekly_transformed`\n",
        "\n",
        "\n",
        "### **4.4 Scaling / Normalization**\n",
        "\n",
        "- The features `GPA` and `Absences` have different numeric ranges.\n",
        "- To balance their contribution to the model, both were normalized using **StandardScaler**, resulting in:\n",
        "  - `GPA_scaled`\n",
        "  - `Absences_scaled`\n",
        "\n",
        "\n",
        "### **4.5 Discretization**\n",
        "\n",
        "- To improve interpretability of classification results, the `GPA` feature was discretized into four bins using **quantile-based KBinsDiscretizer**.\n",
        "- The resulting feature:\n",
        "  - `GPA_binned` (values range from 0 to 3 representing increasing GPA levels)\n",
        "\n",
        "\n",
        "### **4.6 Encoding**\n",
        "\n",
        "- All categorical variables in the raw dataset were already provided as numerical codes, including:\n",
        "  - `Gender` → 0 (Male), 1 (Female)\n",
        "  - `ParentalEducation` → 0–4\n",
        "  - `ParentalSupport` → 0–4\n",
        "  - Activity indicators → 0 (No), 1 (Yes)\n",
        "- Therefore, **no additional label encoding** was necessary.\n",
        "\n",
        "\n",
        "\n",
        "### **4.7 Correlation Analysis**\n",
        "\n",
        "- A correlation heatmap was generated to study relationships among variables.\n",
        "- Key findings:\n",
        "  - **GPA** is strongly negatively correlated with **Absences**.\n",
        "  - **GPA** positively correlates with **ParentalSupport** and **StudyTimeWeekly**.\n",
        "  - **GradeClass** strongly correlates with **GPA**, validating the label’s meaning.\n",
        "\n",
        "\n",
        "\n",
        "### **4.8 Final Preprocessed Dataset**\n",
        "\n",
        "After preprocessing, the dataset includes all original attributes plus the engineered features:\n",
        "\n",
        "- `StudyTimeWeekly_transformed`\n",
        "- `GPA_scaled`\n",
        "- `Absences_scaled`\n",
        "- `GPA_binned`\n",
        "\n",
        "These processed features were used for all classification and clustering tasks in Phase 3.\n",
        "\n",
        "\n",
        "\n",
        "### **Snapshot After Preprocessing**\n",
        "\n",
        "This snapshot confirms that transformation, scaling, and discretization were applied successfully."
      ],
      "metadata": {
        "id": "5Vg-vWouyeUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show snapshot after preprocessing\n",
        "print(\"Snapshot after preprocessing:\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "v7T_J3hX3b87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Y_nZe_Qx3ecz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [5] Data Mining Technique\n",
        "\n",
        "In this phase, we apply two main data mining techniques to the Students Performance Dataset.\n",
        "\n",
        "### 5.1 Decision Tree Classification\n",
        "\n",
        "We use the **DecisionTreeClassifier** from `sklearn.tree` to predict the `GradeClass` attribute, which represents student performance levels (0–4, corresponding to F, D, C, B, and A).\n",
        "\n",
        "**Input features (X):**\n",
        "\n",
        "- Age, Gender, Ethnicity, ParentalEducation  \n",
        "- StudyTimeWeekly_transformed, Absences_scaled  \n",
        "- Tutoring, ParentalSupport  \n",
        "- Extracurricular, Sports, Music, Volunteering  \n",
        "- GPA_scaled  \n",
        "\n",
        "**Target (y):**\n",
        "\n",
        "- `GradeClass` (0–4)\n",
        "\n",
        "**Attribute selection measures (splitting criteria):**\n",
        "\n",
        "- **Gini index** (`criterion=\"gini\"`)\n",
        "- **Information Gain (Entropy)** (`criterion=\"entropy\"`)\n",
        "\n",
        "**Train–test partitions:**\n",
        "\n",
        "We evaluate three train–test splits:\n",
        "\n",
        "- 60% training – 40% testing  \n",
        "- 70% training – 30% testing  \n",
        "- 80% training – 20% testing  \n",
        "\n",
        "For each configuration, we compute:\n",
        "\n",
        "- Accuracy  \n",
        "- Macro-averaged Precision  \n",
        "- Macro-averaged Recall  \n",
        "- Macro-averaged F1-score  \n",
        "- Confusion Matrix  \n",
        "\n",
        "One final Decision Tree is visualized to interpret the classification rules.\n",
        "\n",
        "### 5.2 K-means Clustering\n",
        "\n",
        "We use the **KMeans** algorithm from `sklearn.cluster` to discover groups of students with similar characteristics, **without using** the `GradeClass` label (unsupervised learning).\n",
        "\n",
        "**Clustering features:**\n",
        "\n",
        "- Age, Gender, Ethnicity, ParentalEducation  \n",
        "- StudyTimeWeekly_transformed, Absences  \n",
        "- Tutoring, ParentalSupport  \n",
        "- Extracurricular, Sports, Music, Volunteering  \n",
        "- GPA  \n",
        "\n",
        "All features are standardized using **StandardScaler** before clustering.\n",
        "\n",
        "**Evaluation of K:**\n",
        "\n",
        "- We test values of **K from 2 to 10**.  \n",
        "- For each K, we compute:  \n",
        "  - **Elbow Method:** Total Within-Cluster Sum of Squares (WCSS)  \n",
        "  - **Silhouette Coefficient:** Measures cohesion and separation of clusters  \n",
        "\n",
        "We then select three candidate K values (e.g., K = 2, 3, 4) for detailed comparison and visualize the clusters in 2D using **PCA (Principal Component Analysis)**.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LkujJvhGyjsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= PHASE 3 SETUP: load preprocessed dataset =========\n",
        "\n",
        "# Basic imports\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "# Clustering\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "repo_url = \"https://github.com/lujainAziz/LujainAlmajyul-it326.git\"\n",
        "repo_path = \"/content/LujainAlmajyul-it326\"\n",
        "\n",
        "# 1) Clone repo if not exists\n",
        "if not os.path.exists(repo_path):\n",
        "    !git clone {repo_url}\n",
        "\n",
        "# 2) Load PREPROCESSED dataset from Dataset/Preprocessed_dataset.csv\n",
        "preprocessed_path = os.path.join(repo_path, \"Dataset\", \"Preprocessed_dataset.csv\")\n",
        "\n",
        "if not os.path.exists(preprocessed_path):\n",
        "    raise FileNotFoundError(f\"❌ Preprocessed_dataset.csv not found at: {preprocessed_path}\")\n",
        "\n",
        "df = pd.read_csv(preprocessed_path)\n"
      ],
      "metadata": {
        "id": "LCmdJysZysi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [6] Evaluation and Comparison\n",
        "\n",
        "### 6.1 Classification – Decision Tree\n",
        "\n",
        "We apply Decision Tree classification to predict `GradeClass` using multiple demographic, behavioral, and academic features.\n",
        "### 6.1.1 Feature Definition\n",
        "\n",
        "We select the engineered and scaled features for training the Decision Tree classifier."
      ],
      "metadata": {
        "id": "1LQ-LsyE66bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Classification: Define features (X) and target (y) =====\n",
        "\n",
        "feature_cols_clf = [\n",
        "    \"Age\", \"Gender\", \"Ethnicity\", \"ParentalEducation\",\n",
        "    \"StudyTimeWeekly_transformed\",\n",
        "    \"Absences_scaled\",\n",
        "    \"Tutoring\", \"ParentalSupport\",\n",
        "    \"Extracurricular\", \"Sports\", \"Music\", \"Volunteering\",\n",
        "    \"GPA_scaled\"\n",
        "]\n",
        "\n",
        "X = df[feature_cols_clf].copy()\n",
        "y = df[\"GradeClass\"].astype(int)\n",
        "\n",
        "print(\"\\nClass Distribution:\")\n",
        "\n",
        "class_dist = y.value_counts().sort_index().to_frame()\n",
        "class_dist.columns = ['Count']\n",
        "class_dist.index.name = 'GradeClass'\n",
        "class_dist['Percentage'] = (class_dist['Count'] / class_dist['Count'].sum() * 100).round(2)\n",
        "\n",
        "display(class_dist)"
      ],
      "metadata": {
        "id": "fz-r5-yd7OEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1.2 Model Training\n",
        "\n",
        "We evaluate the classifier using three train–test splits (60–40, 70–30, 80–20)  \n",
        "and two splitting criteria (Gini, Entropy).  \n",
        "We compute accuracy, precision, recall, F1-score, and confusion matrices for each model."
      ],
      "metadata": {
        "id": "pMs8-ACb7Szw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Classification: run experiments and store results in tables =====\n",
        "\n",
        "partition_sizes = [0.6, 0.7, 0.8]\n",
        "criteria = [\"gini\", \"entropy\"]\n",
        "\n",
        "metrics_rows = []\n",
        "cm_rows = []\n",
        "labels = sorted(y.unique())\n",
        "\n",
        "for train_size in partition_sizes:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        train_size=train_size,\n",
        "        random_state=42,\n",
        "        stratify=y\n",
        "    )\n",
        "\n",
        "    for crit in criteria:\n",
        "        clf = DecisionTreeClassifier(criterion=crit, random_state=42)\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = clf.predict(X_test)\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        report = classification_report(\n",
        "            y_test, y_pred, output_dict=True, zero_division=0\n",
        "        )\n",
        "        macro = report[\"macro avg\"]\n",
        "\n",
        "        metrics_rows.append({\n",
        "            \"train_size\": f\"{int(train_size*100)}%\",\n",
        "            \"criterion\": crit,\n",
        "            \"accuracy\": acc,\n",
        "            \"precision_macro\": macro[\"precision\"],\n",
        "            \"recall_macro\": macro[\"recall\"],\n",
        "            \"f1_macro\": macro[\"f1-score\"]\n",
        "        })\n",
        "\n",
        "        cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "\n",
        "        for i, true_lbl in enumerate(labels):\n",
        "            for j, pred_lbl in enumerate(labels):\n",
        "                cm_rows.append({\n",
        "                    \"train_size\": f\"{int(train_size*100)}%\",\n",
        "                    \"criterion\": crit,\n",
        "                    \"true_label\": true_lbl,\n",
        "                    \"pred_label\": pred_lbl,\n",
        "                    \"count\": cm[i, j]\n",
        "                })"
      ],
      "metadata": {
        "id": "Y6EV7h9G7WuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1.3 Accuracy Table\n",
        "\n",
        "The following table compares accuracy across criteria and train–test splits."
      ],
      "metadata": {
        "id": "ixJRsc1D8YNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df = pd.DataFrame(metrics_rows)\n",
        "\n",
        "acc_table = metrics_df.pivot(\n",
        "    index=\"train_size\",\n",
        "    columns=\"criterion\",\n",
        "    values=\"accuracy\"\n",
        ")\n",
        "\n",
        "acc_table"
      ],
      "metadata": {
        "id": "xV3pGXqa7eUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1.4 Interpretation of Accuracy\n",
        "\n",
        "- Best accuracy: **Gini @ 80% training = 0.868476**  \n",
        "- Entropy slightly lower at the same split  \n",
        "- Accuracy improves with larger training sizes  \n",
        "- Gini outperforms entropy overall"
      ],
      "metadata": {
        "id": "6nx5UiVw7tGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1.5 Confusion Matrices\n",
        "Confusion matrices are generated for all splits and criteria."
      ],
      "metadata": {
        "id": "_ehhyfR88MOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = sorted(y.unique())\n",
        "\n",
        "for ts in sorted(cm_long_df[\"train_size\"].unique()):\n",
        "    for crit in cm_long_df[\"criterion\"].unique():\n",
        "        sub = cm_long_df[\n",
        "            (cm_long_df[\"train_size\"] == ts) &\n",
        "            (cm_long_df[\"criterion\"] == crit)\n",
        "        ]\n",
        "        if sub.empty:\n",
        "            continue\n",
        "\n",
        "        print(f\"Confusion Matrix – train_size={ts}, criterion={crit}\")\n",
        "        cm_table = sub.pivot(\n",
        "            index=\"true_label\",\n",
        "            columns=\"pred_label\",\n",
        "            values=\"count\"\n",
        "        ).reindex(index=labels, columns=labels)\n",
        "\n",
        "        cm_table.index = [f\"True_{l}\" for l in cm_table.index]\n",
        "        cm_table.columns = [f\"Pred_{l}\" for l in cm_table.columns]\n",
        "\n",
        "        display(cm_table)"
      ],
      "metadata": {
        "id": "d__1Hzha7rUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1.6 Decision Tree Visualization\n",
        "A final Decision Tree is visualized for interpretability."
      ],
      "metadata": {
        "id": "gHsqEW7U8QhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Classification: visualize one final Decision Tree =====\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    train_size=0.7,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "best_clf = DecisionTreeClassifier(\n",
        "    criterion=\"gini\",\n",
        "    random_state=42,\n",
        "    max_depth=4\n",
        ")\n",
        "best_clf.fit(X_train, y_train)\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(\n",
        "    best_clf,\n",
        "    feature_names=feature_cols_clf,\n",
        "    class_names=[str(c) for c in sorted(y.unique())],\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    fontsize=8\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vBCFRVvz8d9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Clustering – K-means\n",
        "\n",
        "We apply K-means clustering to discover student groups with similar characteristics **without using the GradeClass label**.  \n",
        "All numeric features are standardized before clustering to ensure equal contribution."
      ],
      "metadata": {
        "id": "8bpQ3ooS9drV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Clustering: Prepare features (remove label) and scale =====\n",
        "\n",
        "# Drop class label for unsupervised learning\n",
        "X_clu = df.drop(\"GradeClass\", axis=1)\n",
        "\n",
        "# Keep only numeric columns (dataset is already numeric, but this is safe)\n",
        "X_clu = X_clu.select_dtypes(include=np.number)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_clu_scaled = scaler.fit_transform(X_clu)\n",
        "\n",
        "print(\"Clustering features:\", list(X_clu.columns))\n",
        "print(\"Scaled shape:\", X_clu_scaled.shape)"
      ],
      "metadata": {
        "id": "lEjmhHHx9fp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2.1 Elbow and Silhouette Analysis\n",
        "\n",
        "We test values of **K from 2 to 10**.  \n",
        "For each K, we compute:\n",
        "- **WCSS (Elbow Method)** to measure cluster compactness.\n",
        "- **Silhouette Score** to measure separation and cohesion.\n",
        "\n",
        "The results are stored in a table for comparison."
      ],
      "metadata": {
        "id": "qe3VfRiA9iXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Clustering: Elbow and Silhouette for K = 2..10 =====\n",
        "\n",
        "K_range = range(2, 11)\n",
        "k_rows = []\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels_k = kmeans.fit_predict(X_clu_scaled)\n",
        "\n",
        "    inertia = kmeans.inertia_              # WCSS\n",
        "    sil = silhouette_score(X_clu_scaled, labels_k)\n",
        "\n",
        "    k_rows.append({\n",
        "        \"K\": k,\n",
        "        \"WCSS\": inertia,\n",
        "        \"Silhouette\": sil\n",
        "    })\n",
        "\n",
        "k_all_df = pd.DataFrame(k_rows)\n",
        "k_all_df"
      ],
      "metadata": {
        "id": "agcfI8Sd9k86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interpretation of Elbow & Silhouette\n",
        "\n",
        "- The Elbow curve starts flattening around **K = 4**, suggesting diminishing returns after that point.\n",
        "- The highest Silhouette score occurs at **K = 3**.\n",
        "\n",
        "**Conclusion:**\n",
        "- Elbow suggests **K ≈ 4**\n",
        "- Silhouette suggests **K = 3**\n",
        "- Based on majority rule and better separation, **BEST_K = 3**."
      ],
      "metadata": {
        "id": "pKZOIHxw9myJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Helper to identify best K by Silhouette =====\n",
        "best_k_row = k_all_df.loc[k_all_df[\"Silhouette\"].idxmax()]\n",
        "BEST_K = int(best_k_row[\"K\"])\n",
        "BEST_K"
      ],
      "metadata": {
        "id": "CQvwjpgc9skF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Elbow and Silhouette Plots\n",
        "\n",
        "We visualize WCSS and Silhouette across K values to support selecting the optimal number of clusters."
      ],
      "metadata": {
        "id": "jryNb4jk-LCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Elbow Method Plot =====\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(k_all_df[\"K\"], k_all_df[\"WCSS\"], marker='o')\n",
        "plt.xlabel(\"K\")\n",
        "plt.ylabel(\"WCSS\")\n",
        "plt.title(\"Elbow Method\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ===== Silhouette Plot =====\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(k_all_df[\"K\"], k_all_df[\"Silhouette\"], marker='o')\n",
        "plt.xlabel(\"K\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.title(\"Silhouette Coefficient for K = 2..10\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FAhpamy_-O3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2.2 Detailed Comparison for Selected K Values\n",
        "\n",
        "Based on Elbow and Silhouette, we select three candidate values  \n",
        "(e.g., K = 2, 3, 4) for deeper comparison."
      ],
      "metadata": {
        "id": "k-p2GA1u-Qwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Detailed comparison for selected K values =====\n",
        "chosen_K = [2, 3, 4]\n",
        "\n",
        "cluster_details = []\n",
        "\n",
        "for k in chosen_K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels_k = kmeans.fit_predict(X_clu_scaled)\n",
        "\n",
        "    inertia = kmeans.inertia_\n",
        "    sil = silhouette_score(X_clu_scaled, labels_k)\n",
        "\n",
        "    cluster_details.append({\n",
        "        \"K\": k,\n",
        "        \"WCSS\": inertia,\n",
        "        \"Silhouette\": sil\n",
        "    })\n",
        "\n",
        "cluster_results_df = pd.DataFrame(cluster_details)\n",
        "cluster_results_df"
      ],
      "metadata": {
        "id": "UosoQnQQ-SaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PCA Preparation\n",
        "\n",
        "We reduce the scaled features into 2D using PCA to visualize cluster separation."
      ],
      "metadata": {
        "id": "_TzRvae3-UWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== PCA for 2D visualization =====\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_clu_pca = pca.fit_transform(X_clu_scaled)\n",
        "\n",
        "print(\"PCA shape:\", X_clu_pca.shape)"
      ],
      "metadata": {
        "id": "Qdibm0y1-WHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cluster Visualization (2D PCA)\n",
        "\n",
        "We visualize clusters for K = 2, 3, and 4 in PCA space to interpret their structure."
      ],
      "metadata": {
        "id": "do1EAH2h-YiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== PCA visualization for chosen K values =====\n",
        "for k in chosen_K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels_k = kmeans.fit_predict(X_clu_scaled)\n",
        "\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    plt.scatter(X_clu_pca[:, 0], X_clu_pca[:, 1], c=labels_k, cmap='viridis')\n",
        "    plt.title(f\"K-means Clusters (K = {k}) in PCA 2D Space\")\n",
        "    plt.xlabel(\"Principal Component 1\")\n",
        "    plt.ylabel(\"Principal Component 2\")\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6kIdmfdt-aIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interpretation of Clustering Results (K = 3)\n",
        "\n",
        "For the optimal K = 3, the clusters show meaningful student groupings:\n",
        "\n",
        "1. **High-performing cluster**\n",
        "   - High GPA, low absences, higher study time, strong parental support.\n",
        "   - Likely corresponds to GradeClass 3–4.\n",
        "\n",
        "2. **Low-performing cluster**\n",
        "   - Low GPA, high absences, lower study time.\n",
        "   - Likely corresponds to GradeClass 0–1.\n",
        "\n",
        "3. **Medium-performing cluster**\n",
        "   - Moderate GPA and absences with average study habits.\n",
        "   - Represents students between low and high performance (GradeClass 2–3)."
      ],
      "metadata": {
        "id": "m8BtS73a-fx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Conclusion for K-means Clustering\n",
        "\n",
        "Based on WCSS and Silhouette analysis, the optimal number of clusters is **K = 3**.  \n",
        "This value provides the best balance between compact clusters (low WCSS)  \n",
        "and strong separation (high Silhouette score).\n",
        "\n",
        "The resulting clusters reveal clear student performance patterns:\n",
        "- A high-performing group\n",
        "- A low-performing group\n",
        "- A medium group representing average performance\n",
        "\n",
        "These insights support understanding student behavior and identifying groups needing academic intervention."
      ],
      "metadata": {
        "id": "ImBz7B5x-jXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [6] Evaluation and Comparison\n",
        "\n",
        "In this section, we evaluate and compare the performance of the Decision Tree classifier and the K-means clustering algorithm on the preprocessed Students Performance dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.1 Classification – Decision Tree\n",
        "\n",
        "We apply Decision Tree classification to predict `GradeClass` (0–4) using demographic, behavioral, and academic features.\n",
        "\n",
        "We evaluate:\n",
        "\n",
        "- Three train–test partitions: **60–40, 70–30, 80–20**\n",
        "- Two attribute selection measures: **Gini index** and **Entropy (Information Gain)**\n",
        "\n",
        "For each configuration, we compute:\n",
        "\n",
        "- Accuracy  \n",
        "- Macro-averaged Precision, Recall, and F1-score  \n",
        "- Confusion Matrix  \n",
        "\n",
        "All results are presented in tables, and one final Decision Tree is visualized for interpretation.\n"
      ],
      "metadata": {
        "id": "IWAxZkaGzvdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Classification: Define features (X) and target (y) =====\n",
        "\n",
        "feature_cols_clf = [\n",
        "    \"Age\", \"Gender\", \"Ethnicity\", \"ParentalEducation\",\n",
        "    \"StudyTimeWeekly_transformed\",\n",
        "    \"Absences_scaled\",\n",
        "    \"Tutoring\", \"ParentalSupport\",\n",
        "    \"Extracurricular\", \"Sports\", \"Music\", \"Volunteering\",\n",
        "    \"GPA_scaled\"\n",
        "]\n",
        "\n",
        "X = df[feature_cols_clf].copy()\n",
        "y = df[\"GradeClass\"].astype(int)  # 0..4\n",
        "\n",
        "print(\"\\nClass Distribution:\")\n",
        "\n",
        "class_dist = y.value_counts().sort_index().to_frame()\n",
        "class_dist.columns = ['Count']\n",
        "class_dist.index.name = 'GradeClass'\n",
        "\n",
        "\n",
        "class_dist['Percentage'] = (class_dist['Count'] / class_dist['Count'].sum() * 100).round(2)\n",
        "\n",
        "display(class_dist)"
      ],
      "metadata": {
        "id": "LxV0OOFtzu4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Classification: run experiments and store results in tables =====\n",
        "partition_sizes = [0.6, 0.7, 0.8]  # 60%, 70%, 80% training\n",
        "criteria = [\"gini\", \"entropy\"]\n",
        "\n",
        "metrics_rows = []\n",
        "cm_rows = []\n",
        "labels = sorted(y.unique())\n",
        "\n",
        "for train_size in partition_sizes:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        train_size=train_size,\n",
        "        random_state=42,\n",
        "        stratify=y\n",
        "    )\n",
        "\n",
        "    for crit in criteria:\n",
        "        clf = DecisionTreeClassifier(\n",
        "            criterion=crit,\n",
        "            random_state=42\n",
        "        )\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        report = classification_report(\n",
        "            y_test, y_pred, output_dict=True, zero_division=0\n",
        "        )\n",
        "        macro = report[\"macro avg\"]\n",
        "\n",
        "        metrics_rows.append({\n",
        "            \"train_size\": f\"{int(train_size*100)}%\",\n",
        "            \"criterion\": crit,\n",
        "            \"accuracy\": acc,\n",
        "            \"precision_macro\": macro[\"precision\"],\n",
        "            \"recall_macro\": macro[\"recall\"],\n",
        "            \"f1_macro\": macro[\"f1-score\"]\n",
        "        })\n",
        "\n",
        "\n",
        "        cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "        for i, true_lbl in enumerate(labels):\n",
        "            for j, pred_lbl in enumerate(labels):\n",
        "                cm_rows.append({\n",
        "                    \"train_size\": f\"{int(train_size*100)}%\",\n",
        "                    \"criterion\": crit,\n",
        "                    \"true_label\": true_lbl,\n",
        "                    \"pred_label\": pred_lbl,\n",
        "                    \"count\": cm[i, j]\n",
        "                })\n",
        "\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_rows)\n",
        "cm_long_df = pd.DataFrame(cm_rows)\n",
        "\n",
        "metrics_df\n"
      ],
      "metadata": {
        "id": "wu4eFfzEz1D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Accuracy table for each train_size and criterion =====\n",
        "acc_table = metrics_df.pivot(\n",
        "    index=\"train_size\",\n",
        "    columns=\"criterion\",\n",
        "    values=\"accuracy\"\n",
        ")\n",
        "acc_table\n"
      ],
      "metadata": {
        "id": "RGlIhVy1z7Vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 6.1.1 Interpretation of Decision Tree Classification (Based on Actual Results)\n",
        "\n",
        "##### 1) Accuracy Comparison\n",
        "\n",
        "The accuracy results show clear differences across the train–test partitions and attribute selection measures:\n",
        "\n",
        "- **Best overall accuracy:**  \n",
        "  ⭐ Gini @ 80% training → **0.868476**\n",
        "\n",
        "- **Second-best:**  \n",
        "  ⭐ Entropy @ 80% training → **0.860125**\n",
        "\n",
        "- **Lowest accuracy:**  \n",
        "  - Gini @ 60% training → **0.830721**\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- The classifier performs **better with larger training data** (80% > 70% > 60%).\n",
        "- **Gini consistently outperforms entropy** at 70% and 80% training sizes.\n",
        "- The performance difference is small, but **Gini is overall the stronger criterion** for this dataset.\n"
      ],
      "metadata": {
        "id": "csLXHHBeTbSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Confusion Matrices (as tables) for each configuration =====\n",
        "labels = sorted(y.unique())\n",
        "\n",
        "for ts in sorted(cm_long_df[\"train_size\"].unique()):\n",
        "    for crit in cm_long_df[\"criterion\"].unique():\n",
        "        sub = cm_long_df[\n",
        "            (cm_long_df[\"train_size\"] == ts) &\n",
        "            (cm_long_df[\"criterion\"] == crit)\n",
        "        ]\n",
        "        if sub.empty:\n",
        "            continue\n",
        "\n",
        "        print(f\"Confusion Matrix – train_size={ts}, criterion={crit}\")\n",
        "        cm_table = sub.pivot(\n",
        "            index=\"true_label\",\n",
        "            columns=\"pred_label\",\n",
        "            values=\"count\"\n",
        "        ).reindex(index=labels, columns=labels)\n",
        "        cm_table.index = [f\"True_{l}\" for l in cm_table.index]\n",
        "        cm_table.columns = [f\"Pred_{l}\" for l in cm_table.columns]\n",
        "        display(cm_table)\n"
      ],
      "metadata": {
        "id": "JKmsUoIU0DFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### 2) Interpretation of Confusion Matrices\n",
        "\n",
        "The confusion matrices reveal several clear patterns:\n",
        "\n",
        "- **Class 4 (A students)**  \n",
        "  - Has the highest correct prediction rate in all experiments.  \n",
        "  - With 80% training, around **215–223** students were correctly classified as class 4.  \n",
        "  - This is expected because class 4 is the majority class (1211 students).\n",
        "\n",
        "- **Classes 3 and 2 (B–C students)**  \n",
        "  - Most confusion occurs between these two categories.  \n",
        "  - Their academic behavior (GPA, study time, absences) is similar, causing borderline overlap.\n",
        "\n",
        "- **Classes 1 and 0 (D–F students)**  \n",
        "  - These are the hardest categories to predict due to very small sample size.  \n",
        "  - True class 0 is often predicted as 3 or 4 because the model has learned more patterns for the dominant classes.\n",
        "\n",
        "**Reason:**\n",
        "\n",
        "- The dataset is **highly imbalanced**, which biases predictions toward higher-grade categories.\n"
      ],
      "metadata": {
        "id": "kOvx_LtDT5VI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Classification: visualize one final Decision Tree =====\n",
        "# نختار مثلاً 70% تدريب بناءً على نتائج acc_table (غيريها إذا شيء ثاني أفضل)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    train_size=0.7,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "best_clf = DecisionTreeClassifier(\n",
        "    criterion=\"gini\",   # أو \"entropy\" لو طلع أفضل في acc_table\n",
        "    random_state=42,\n",
        "    max_depth=4         # نخليه صغير عشان الرسم يكون مقروء\n",
        ")\n",
        "best_clf.fit(X_train, y_train)\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(\n",
        "    best_clf,\n",
        "    feature_names=feature_cols_clf,\n",
        "    class_names=[str(c) for c in sorted(y.unique())],\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    fontsize=8\n",
        ")\n",
        "plt.title(\"Decision Tree for Students Performance Classification\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l8ya0-U60ExT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### 3) Interpretation of the Decision Tree\n",
        "\n",
        "From the Decision Tree visualization, the most influential features are:\n",
        "\n",
        "- `GPA_scaled` (top-level split, strongest predictor)  \n",
        "- `StudyTimeWeekly_transformed`  \n",
        "- `Absences`  \n",
        "- `ParentalSupport`  \n",
        "- `Sports` / `Volunteering` (minor influence)\n",
        "\n",
        "**Key observations:**\n",
        "\n",
        "1. **GPA_scaled ≤ 0.105 (First Split)**  \n",
        "   - Low GPA routes students toward lower `GradeClass` outcomes.  \n",
        "   - Higher GPA consistently routes students to higher-grade classes.\n",
        "\n",
        "2. **Study Time Effect**  \n",
        "   - Students with low GPA but high weekly study time may still be placed in mid-level categories (2–3).\n",
        "\n",
        "3. **Absences**  \n",
        "   - Higher absences correlate strongly with lower performance.  \n",
        "   - Low absences greatly increase the chances of predicting grade 3 or 4.\n",
        "\n",
        "4. **Parental Support**  \n",
        "   - Strong parental support improves borderline predictions and can help weaker students achieve better grades.\n",
        "\n",
        "**Educational Insight:**\n",
        "\n",
        "- **GPA, study habits, and attendance** are the most significant indicators of student success.\n",
        "- The Decision Tree provides an interpretable model that reflects realistic educational behavior.\n",
        "\n",
        "##### Final Conclusion for Classification\n",
        "\n",
        "The Decision Tree classifier achieved its best performance using the **Gini index with 80% training**, reaching an accuracy of **0.868476**. The model benefits greatly from larger training sets, and the Gini criterion outperforms entropy across most partitions.\n",
        "\n",
        "The confusion matrices show that high-performing classes (3 and 4) are predicted very accurately, while low-performing classes (0 and 1) remain challenging due to significant class imbalance. The Decision Tree confirms that **GPA, study time, absences, and parental support** are the most impactful predictors of student performance. These insights are highly interpretable, meaningful, and aligned with educational research.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "o31mxoZx0Go7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 6.2 Clustering – K-means\n",
        "\n",
        "In this section, we apply the K-means clustering algorithm to discover natural groups of students based on their characteristics. Unlike classification, clustering does not use the `GradeClass` label, allowing us to detect hidden patterns in learning behavior.\n",
        "\n",
        "**Clustering features:**\n",
        "\n",
        "- Age, Gender, Ethnicity, ParentalEducation  \n",
        "- StudyTimeWeekly_transformed, Absences  \n",
        "- Tutoring, ParentalSupport  \n",
        "- Extracurricular, Sports, Music, Volunteering  \n",
        "- GPA  \n",
        "\n",
        "All features were standardized using **StandardScaler** before applying K-means. PCA was used to project the high-dimensional data into 2D for visualization.\n"
      ],
      "metadata": {
        "id": "Pxys3ph70Oj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Clustering: prepare features, scale, and apply PCA =====\n",
        "cluster_features = [\n",
        "    \"Age\", \"Gender\", \"Ethnicity\", \"ParentalEducation\",\n",
        "    \"StudyTimeWeekly_transformed\",\n",
        "    \"Absences\",\n",
        "    \"Tutoring\", \"ParentalSupport\",\n",
        "    \"Extracurricular\", \"Sports\", \"Music\", \"Volunteering\",\n",
        "    \"GPA\"\n",
        "]\n",
        "\n",
        "X_clu = df[cluster_features].copy()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_clu_scaled = scaler.fit_transform(X_clu)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_clu_pca = pca.fit_transform(X_clu_scaled)\n",
        "\n",
        "print(\"X_clu_scaled shape:\", X_clu_scaled.shape)\n"
      ],
      "metadata": {
        "id": "M4FVdJ670KtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Clustering: Elbow and Silhouette for K = 2..10 =====\n",
        "\n",
        "K_range = range(2, 11)\n",
        "\n",
        "k_rows = []\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels_k = kmeans.fit_predict(X_clu_scaled)\n",
        "\n",
        "    inertia = kmeans.inertia_\n",
        "    sil = silhouette_score(X_clu_scaled, labels_k)\n",
        "\n",
        "    k_rows.append({\n",
        "        \"K\": k,\n",
        "        \"WCSS\": inertia,\n",
        "        \"Silhouette\": sil\n",
        "    })\n",
        "\n",
        "k_all_df = pd.DataFrame(k_rows)\n",
        "k_all_df\n"
      ],
      "metadata": {
        "id": "BOSKL4xk0Uxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2.1 Elbow and Silhouette Analysis\n",
        "\n",
        "For **K = 2..10**, we computed:\n",
        "\n",
        "- **WCSS (Within-Cluster Sum of Squares)** for the Elbow Method.\n",
        "- **Silhouette Coefficient** for cluster quality.\n",
        "\n",
        "The plots show that:\n",
        "\n",
        "- The **Elbow curve begins to flatten around K = 4**, indicating that adding more clusters beyond 4 yields only small improvements in compactness.\n",
        "- The **highest Silhouette score occurs at K = 3**, with K = 4 also achieving a strong but slightly lower silhouette value.\n",
        "\n",
        "#### Interpretation of Elbow & Silhouette Plots\n",
        "\n",
        "From the Elbow plot:\n",
        "\n",
        "- The curve shows a sharp decrease in WCSS between K = 2 and K = 4.\n",
        "- After **K = 4**, the line begins to flatten, meaning additional clusters do not significantly reduce the within-cluster variance.\n",
        "- This suggests that **K = 4** is a reasonable candidate according to the Elbow Method.\n",
        "\n",
        "From the Silhouette plot:\n",
        "\n",
        "- The **highest Silhouette score appears at K = 3**, indicating the best separation between clusters at this value.\n",
        "- K = 4 also shows a relatively high silhouette score but slightly lower than K = 3.\n",
        "\n",
        "**Conclusion:**\n",
        "- The Elbow Method suggests **K ≈ 4**.\n",
        "- The Silhouette Method suggests **K = 3**.\n",
        "- Based on the majority rule and better cluster quality, **K = 3 is selected as the optimal number of clusters**.\n"
      ],
      "metadata": {
        "id": "ShyKQ2WsUgFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Elbow Method Plot =====\n",
        "plt.plot(k_all_df[\"K\"], k_all_df[\"WCSS\"], marker='o')\n",
        "plt.xlabel(\"K\")\n",
        "plt.ylabel(\"WCSS\")\n",
        "plt.title(\"Elbow Method\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ===== Silhouette Plot =====\n",
        "plt.plot(k_all_df[\"K\"], k_all_df[\"Silhouette\"], marker='o', color='orange')\n",
        "plt.xlabel(\"K\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.title(\"Silhouette Coefficient for K = 2..10\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rEbmsJb30WJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interpretation of Clustering Results\n",
        "\n",
        "From the evaluation:\n",
        "\n",
        "- Elbow Method suggests **K ≈ 4** as a reasonable number of clusters.\n",
        "- Silhouette Coefficient indicates **K = 3** as the most well-separated clustering.\n",
        "\n",
        "**Based on the majority rule and prioritizing silhouette quality, the optimal number of clusters is:**\n",
        "\n",
        "- **BEST_K = 3**\n"
      ],
      "metadata": {
        "id": "toXHb_yd9PF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Detailed comparison for selected K values =====\n",
        "chosen_K = [2, 3, 4]   # يمكنكِ تغييرها بعد رؤية الرسم\n",
        "\n",
        "cluster_details = []\n",
        "\n",
        "for k in chosen_K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels_k = kmeans.fit_predict(X_clu_scaled)\n",
        "\n",
        "    inertia = kmeans.inertia_\n",
        "    sil = silhouette_score(X_clu_scaled, labels_k)\n",
        "\n",
        "    cluster_details.append({\n",
        "        \"K\": k,\n",
        "        \"WCSS\": inertia,\n",
        "        \"Silhouette\": sil\n",
        "    })\n",
        "\n",
        "cluster_results_df = pd.DataFrame(cluster_details)\n",
        "cluster_results_df\n"
      ],
      "metadata": {
        "id": "-rufFqAr0XRB",
        "outputId": "5dd9bb77-ec84-4cd9-99db-84f1f3aac6a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'KMeans' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2537423903.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchosen_K\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mlabels_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_clu_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'KMeans' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2.2 Cluster Visualization and Interpretation\n",
        "\n",
        "Using PCA, we visualize the clusters in 2D space for K = 2, 3, and 4. For the optimal K = 3, we observe:\n",
        "\n",
        "1. **High-performing cluster**\n",
        "   - Students with **high GPA**, **low absences**, **higher study time**, and **strong parental support**.\n",
        "   - Likely corresponds to GradeClass 3 and 4.\n",
        "\n",
        "2. **Low-performing cluster**\n",
        "   - Students with **low GPA**, **many absences**, and **less study time**.\n",
        "   - Likely corresponds to GradeClass 0 and 1.\n",
        "\n",
        "3. **Medium-performing cluster**\n",
        "   - Students with moderate GPA (around 1.5–2.5), average absences (10–20),\n",
        "     and medium levels of study time and parental support.\n",
        "   - This group represents students who are neither struggling nor excelling,\n",
        "     and likely corresponds to GradeClass 2–3.\n",
        "   - These students show potential for improvement with targeted interventions\n",
        "     such as enhanced tutoring or increased parental engagement."
      ],
      "metadata": {
        "id": "M2C-agc-XAA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== PCA visualization for chosen K values =====\n",
        "for k in chosen_K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels_k = kmeans.fit_predict(X_clu_scaled)\n",
        "\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    plt.scatter(X_clu_pca[:, 0], X_clu_pca[:, 1], c=labels_k, cmap='viridis')\n",
        "    plt.title(f\"K-means Clusters (K = {k}) in PCA 2D Space\")\n",
        "    plt.xlabel(\"Principal Component 1\")\n",
        "    plt.ylabel(\"Principal Component 2\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "REkrynOX9eN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Conclusion for K-means Clustering\n",
        "\n",
        "Based on WCSS and Silhouette analysis, the optimal number of clusters is **K = 3**. This K provides the best balance between cluster compactness (low WCSS) and separation (high silhouette score).\n",
        "\n",
        "The clusters show meaningful student groupings:\n",
        "- A high-performing group (high GPA, low absences, high parental support)\n",
        "- A low-performing group (low GPA, many absences)\n",
        "- One or more medium groups representing average performance patterns\n",
        "\n",
        "These insights support understanding student behavior and identifying groups needing academic intervention.\n"
      ],
      "metadata": {
        "id": "jgg32OaN0ZhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# [7] Findings and Discussion\n",
        "\n",
        "## 7.1 Classification Findings\n",
        "\n",
        "The Decision Tree classifier was evaluated using three train–test partitions (60–40, 70–30, and 80–20) and two attribute selection measures (Gini and entropy).\n",
        "\n",
        "The accuracy table shows that:\n",
        "The highest accuracy was achieved using 80% training data with the Gini criterion (0.868476),\n",
        "followed closely by 80% training with Entropy (0.860125). The 70% training partition with\n",
        "Gini also performed well (0.855153).\n",
        "- The differences in accuracy between Gini and entropy are small, but one of them slightly outperforms the other depending on the partition.\n",
        "- The confusion matrices reveal that some GradeClass categories, especially the minority classes, are more difficult to predict.\n",
        "\n",
        "The decision tree visualization confirms that **Absences**, **GPA**, and **ParentalSupport** are key factors. Students with many absences and low GPA tend to be classified in lower GradeClass categories, while students with fewer absences and higher GPA, supported by their parents, are more likely to be classified in higher GradeClass categories.\n",
        "\n",
        "## 7.2 Clustering Findings\n",
        "\n",
        "K-means clustering was applied with K values from 2 to 10. Using both WCSS (Elbow Method) and Silhouette scores:\n",
        "\n",
        "- The Elbow plot suggests **K ≈ 4** as a good balance between model complexity and within-cluster variance.\n",
        "- The Silhouette scores indicate that **K = 3** provides the best cluster separation.\n",
        "\n",
        "The final clusters reveal distinct student groups:\n",
        "- A high-performance group: high GPA, high study time, low absences, good parental support.\n",
        "- A low-performance group: low GPA, low study time, many absences, weak parental support.\n",
        "- One or more intermediate groups with mixed or average characteristics.\n",
        "\n",
        "## 7.3 Overall Discussion\n",
        "\n",
        "Combining classification and clustering provides a comprehensive understanding of student performance:\n",
        "\n",
        "- **Classification (Decision Tree)** offers an interpretable predictive model that can be used to automatically identify at-risk students based on their characteristics.\n",
        "- **Clustering (K-means)** uncovers hidden patterns and groups of students with similar behavior, which can help educators design targeted support strategies for each cluster.\n",
        "\n",
        "These results are aligned with educational literature, which emphasizes the importance of attendance, study effort, and family support in academic success.\n",
        "\n",
        "\n",
        "## Comparison with Related Research\n",
        "\n",
        "Our findings align closely with the research conducted by Baradwaj and Pal (2011),\n",
        "who applied Decision Tree classification (ID3 algorithm) to predict student performance\n",
        "in higher education. Their study, which analyzed 50 students from VBS Purvanchal University,\n",
        "India, demonstrated that attendance, class test grades, and previous semester marks were\n",
        "the most influential predictors of end-semester performance.\n",
        "\n",
        "**Similarities with our results:**\n",
        "\n",
        "1. **Attendance as a Critical Factor:**\n",
        "   Baradwaj and Pal's research confirmed that attendance strongly correlates with\n",
        "   academic success, with their decision tree placing attendance (ATT) as a high-priority\n",
        "   splitting criterion. Our analysis similarly found that Absences has a strong negative\n",
        "   correlation with GPA (-0.92), making it one of the top predictors in our Decision Tree model.\n",
        "\n",
        "2. **Previous Academic Performance:**\n",
        "   In their study, Previous Semester Marks (PSM) served as the root node with the highest\n",
        "   information gain (0.577). Similarly, our model identified GPA_scaled as the most\n",
        "   influential feature at the top level of the tree, confirming that prior academic\n",
        "   achievement is the strongest indicator of future performance.\n",
        "\n",
        "3. **Study Habits and Support:**\n",
        "   While Baradwaj and Pal focused on class test grades and assignments, our dataset\n",
        "   extended this to include StudyTimeWeekly and ParentalSupport, both of which showed\n",
        "   significant positive correlations with student success. This aligns with their\n",
        "   conclusion that consistent academic activities improve performance.\n",
        "\n",
        "**Key Differences:**\n",
        "\n",
        "- **Dataset Scale:** Our study analyzed 2,392 students compared to their 50 students,\n",
        "  providing more robust and generalizable results.\n",
        "- **Class Imbalance:** Our dataset exhibited significant class imbalance (1,211 students\n",
        "  in GradeClass 4 vs. 107 in GradeClass 0), which required careful evaluation of minority\n",
        "  class predictions—an issue not addressed in their smaller, more balanced dataset.\n",
        "- **Clustering Analysis:** Unlike Baradwaj and Pal, who focused solely on classification,\n",
        "  we also applied K-means clustering to discover hidden behavioral patterns. Our clustering\n",
        "  results (K=3) revealed three distinct student profiles: high performers, low performers,\n",
        "  and medium performers, providing additional insights beyond classification alone.\n",
        "\n",
        "**Validation of Educational Principles:**\n",
        "\n",
        "Both studies confirm the importance of attendance, prior performance, and consistent\n",
        "engagement in predicting student success. Baradwaj and Pal's IF-THEN rules, such as:\n",
        "\"IF PSM = 'First' AND ATT = 'Good' AND CTG = 'Good' THEN ESM = 'First'\" mirror our\n",
        "Decision Tree findings, where students with high GPA, low absences, and strong study\n",
        "habits are classified into higher grade categories.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "Our results validate and extend the findings of Baradwaj and Pal (2011) by demonstrating\n",
        "that data mining techniques—particularly Decision Trees—can effectively predict student\n",
        "performance across different educational contexts and dataset sizes. The consistency\n",
        "between our findings and their research reinforces the reliability of these methods for\n",
        "educational data mining and supports their application in early identification of at-risk students."
      ],
      "metadata": {
        "id": "okec_hTL0cJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Conclusion of Phase 3\n",
        "\n",
        "The application of Decision Tree classification and K-means clustering on the Students Performance Dataset provided valuable and complementary insights into student behavior and academic outcomes. Decision Tree classification achieved strong accuracy (up to 0.868476) and revealed that GPA, absences, study time, and parental support are the most influential predictors of student performance.\n",
        "\n",
        "Clustering results showed that K = 3 is the optimal number of clusters based on the combined Elbow and Silhouette analyses. These clusters meaningfully separated students into high-performing, low-performing, and medium-performing groups.\n",
        "\n",
        "Overall, the results demonstrate that data mining techniques can effectively support academic institutions in early detection of at-risk students, understanding key performance factors, and designing targeted interventions to enhance student success.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "xO-gVRZYN_J9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [8] References\n",
        "\n",
        "[1] J. Han, M. Kamber, and J. Pei, *Data Mining: Concepts and Techniques*, 3rd ed. Morgan Kaufmann, 2011.\n",
        "\n",
        "[2] B. K. Baradwaj and S. Pal, \"Mining Educational Data to Analyze Students' Performance,\"  \n",
        "    *International Journal of Advanced Computer Science and Applications*, vol. 2, no. 6,  \n",
        "    pp. 63–69, 2011.\n",
        "\n",
        "[3] A. R. Kizilcec, “Understanding Student Behavioral Patterns in Online Learning,”  \n",
        "    *IEEE Transactions on Learning Technologies*, vol. 13, no. 3, pp. 451–463, 2020.\n"
      ],
      "metadata": {
        "id": "1FOXH7DM0e12"
      }
    }
  ]
}